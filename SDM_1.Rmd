---
title: "**Medical Diagnosis on Exasens Dataset**"
subtitle: "Statistical Learning II | Project 1"
output:
  pdf_document: default
  html_document: default
---

## LIST OF LIBRARIES

```{r}
#Loading required Libraries
library(dplyr)
library(tidyr)
library(caret)
library(softImpute)
library(ggplot2)
library(factoextra)
library(rpart.plot)
library(cluster)
```

## DATA PREPROCESSING

```{r}
# Loading the data
set.seed(90)
data <- read.csv("Exasens.csv", na.strings = c("", "NA"))
head(data)
```

```{r}
# Dropping the ID column
data$ID <- NULL
# Dropping unnecessary columns 
new_data <- which(names(data) == "Smoking")
data <- data[, 1:new_data]
```

```{r}
# Removing the first two rows with Min and Avg labels
data <- data[-c(1, 2), ]

# Renaming columns to represent min and avg for Imaginary and Real parts
data <- data %>%
  rename(
    Imaginary_Part_Min = Imaginary.Part,
    Imaginary_Part_Avg = X,
    Real_Part_Min = Real.Part,
    Real_Part_Avg = X.1
  )
```


```{r}
sum(is.na(data))
```


```{r}
# Display the count of missing values per column to decide on the method
colSums(is.na(data))
```


```{r}
# Converting columns to numeric for matrix completion to fill in missing values
data$Imaginary_Part_Min <- as.numeric(data$Imaginary_Part_Min)
data$Imaginary_Part_Avg <- as.numeric(data$Imaginary_Part_Avg)
data$Real_Part_Min <- as.numeric(data$Real_Part_Min)
data$Real_Part_Avg <- as.numeric(data$Real_Part_Avg)
```

```{r}
# Convert required columns to a factor
data$Diagnosis <- as.factor(data$Diagnosis)
data$Gender <- as.factor(data$Gender)
data$Smoking <- as.factor(data$Smoking)
str(data)
```


```{r}
# Preparing the matrix for completion, selecting only the numeric columns with missing values
matrix_data <- as.matrix(data[, c("Imaginary_Part_Min", "Imaginary_Part_Avg", 
                                  "Real_Part_Min", "Real_Part_Avg")])

# Performing matrix completion
filling_missing_val <- softImpute(matrix_data, rank.max = 3, lambda = 0.1)

# Fill in missing values
filled_data <- complete(matrix_data, filling_missing_val)
```

```{r}
sum(is.na(filled_data))
```


```{r}
# Update the original data frame with filled values
data$Imaginary_Part_Min <- filled_data[, "Imaginary_Part_Min"]
data$Imaginary_Part_Avg <- filled_data[, "Imaginary_Part_Avg"]
data$Real_Part_Min <- filled_data[, "Real_Part_Min"]
data$Real_Part_Avg <- filled_data[, "Real_Part_Avg"]
```


```{r}
# Scale numeric features
numeric_cols <- c("Imaginary_Part_Min", "Imaginary_Part_Avg", "Real_Part_Min", 
                  "Real_Part_Avg", "Age")
data[numeric_cols] <- scale(data[numeric_cols])

# Print the first few rows of the scaled columns
head(data[numeric_cols])
```

```{r}
# Creating new features for differences between Min and Avg
data <- data %>%
  mutate(
    Imaginary_Difference = Imaginary_Part_Avg - Imaginary_Part_Min,
    Real_Difference = Real_Part_Avg - Real_Part_Min
  )

# View the updated data
head(data)
# Structure of the final dataset
str(data)
```

## EXPLORATORY DATA ANALYSIS

```{r, fig.width = 8, fig.height = 6}
#1. Distribution of Age by Diagnosis
library(ggplot2)
ggplot(data, aes(x = Diagnosis, y = Age, fill = Diagnosis)) +
  geom_boxplot() +
  labs(title = "Age Distribution by Diagnosis", x = "Diagnosis", y = "Age") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")   

```

```{r}

# Density plot of Age by Gender
data$Gender <- ifelse(data$Gender == 0, "Female", "Male")

# Density plot of Age by Gender
ggplot(data, aes(x = Age, fill = Gender)) + 
  geom_density(alpha = 0.6) +  # Adjust alpha for transparency
  labs(title = "Density Plot of Age by Gender", x = "Age", y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("pink", "lightblue"))  # Customize colors


```

```{r}
#Smoking Status by Diagnosis
# Change Smoking values (1, 2, 3) to actual labels
data$Smoking <- factor(data$Smoking, levels = c(1, 2, 3), 
                       labels = c("Non-Smoker", "Ex-Smoker", "Active-Smoker"))

# Smoking Status by Diagnosis
# Update Smoking Status by Diagnosis with custom colors
ggplot(data, aes(x = Diagnosis, fill = Smoking)) +
  geom_bar(position = "fill") +
  labs(title = "Smoking Status by Diagnosis", x = "Diagnosis", y = "Proportion") +
  scale_fill_manual(
    values = c("Non-Smoker" = "chartreuse3", "Ex-Smoker" = "steelblue", "Active-Smoker" = "red"),
    name = "Smoking Status"
  ) +
  theme_minimal()


```

```{r}
library(corrplot)

# Calculate the correlation matrix
numeric_data <- data %>% select(where(is.numeric))
corr_matrix <- cor(numeric_data, use = "complete.obs")

# Create the correlation plot
corrplot(corr_matrix, method = "color", type = "upper", tl.cex = 0.4)

# Add a title with proper spacing
title("Correlation Plot of Numeric Variables", line = 2.5)
```


```{r, fig.width=6, fig.height=4}
#Pairplot
library(GGally)
ggpairs(data[c("Imaginary_Part_Min", "Imaginary_Part_Avg", "Real_Part_Min", 
               "Real_Part_Avg", "Imaginary_Difference", "Real_Difference", 
               "Age")], aes(color = data$Gender))
```


```{r, fig.width = 6, fig.height = 4}
# PCA
numeric_data <- data %>% select(where(is.numeric))  # Selecting numeric columns
scaled_data <- scale(numeric_data)  # Standardizing

# Perform PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

# Summary of PCA results
summary(pca_result)

# Plot explained variance
library(ggplot2)
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
ggplot(data.frame(PC = 1:length(explained_variance), 
                  Variance = explained_variance), 
       aes(x = PC, y = Variance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Explained Variance by Principal Components", 
       x = "Principal Component", y = "Variance Explained") + theme_minimal()

# Biplot to visualize PCA results
biplot(pca_result, scale=0, cex=0.5)
```


```{r, fig.width=8, fig.height=4}
#KPCA

# Load the kernlab package
library(kernlab)

# Convert the scaled matrix to a data frame for KPCA
scaled_data_df <- as.data.frame(scaled_data)

# Perform Kernel PCA with a radial basis function (RBF) kernel
kpca_result <- kpca(~., data = scaled_data_df, kernel = "rbfdot", 
                    kpar = list(sigma = 0.1), features = 2)

# Extract the transformed data
kpca_data <- rotated(kpca_result)

# Plot the KPCA result
kpca_df <- data.frame(PC1 = kpca_data[,1], PC2 = kpca_data[,2], 
                      Diagnosis = data$Diagnosis)
ggplot(kpca_df, aes(x = PC1, y = PC2, color = Diagnosis)) +
  geom_point(alpha = 0.7) +
  labs(title = "KPCA Projection", x = "KPCA 1", y = "KPCA 2") +
  theme_minimal()


```

## MODEL BUILDING

```{r}
# Splitting the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(data$Smoking, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

```

### 1. Random Forest
```{r}
library(randomForest)

# Train a Random Forest model
set.seed(123)
rf_model <- randomForest(Diagnosis ~ ., data = trainData, ntree = 100, 
                         mtry = 3, importance = TRUE)

# Predict on test data
rf_pred <- predict(rf_model, newdata = testData)

# Evaluate
conf_mat <- confusionMatrix(rf_pred, testData$Diagnosis)

# Convert the confusion matrix to a table
cm_table <- as.table(conf_mat$table)

# Create a data frame for ggplot2
cm_df <- as.data.frame(cm_table)

# Plot the confusion matrix
ggplot(data = cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix for Random Forest", x = "Predicted", 
       y = "Actual")


# Plot variable importance
varImpPlot(rf_model)

```

### 2. Support Vector Machine
```{r}
library(e1071)
set.seed(123)
# Train an SVM model
svm_model <- svm(Diagnosis ~ ., data = trainData, kernel = "linear", cost = 1)

# Predict on test data
svm_pred <- predict(svm_model, newdata = testData)

# Evaluate
conf_mat <- confusionMatrix(svm_pred, testData$Diagnosis)

# Convert the confusion matrix to a table
cm_table <- as.table(conf_mat$table)

# Create a data frame for ggplot2
cm_df <- as.data.frame(cm_table)

# Plot the confusion matrix
ggplot(data = cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix for SVM", x = "Predicted", y = "Actual")
```

### 3. Multinomial Logistic Regression
```{r}
library(nnet)
library(caret)
set.seed(123)
# Train a Multinomial Logistic Regression model
log_model <- multinom(Diagnosis ~ ., data = trainData)
summary(log_model)

# Predict on test data
log_pred <- predict(log_model, newdata = testData, type = "class")

# Evaluate the model
conf_mat<-confusionMatrix(factor(log_pred, levels = levels(testData$Diagnosis)),
                            testData$Diagnosis)

# Convert the confusion matrix to a table
cm_table <- as.table(conf_mat$table)

# Create a data frame for ggplot2
cm_df <- as.data.frame(cm_table)

# Plot the confusion matrix
ggplot(data = cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix for Multinomial LR", x = "Predicted", 
       y = "Actual")

```

### Decision Tree
```{r}
library(rpart)
library(caret)
set.seed(123)
# Train a Decision Tree model
dt_model <- rpart(Diagnosis ~ ., data = trainData, method = "class")
rpart.plot(dt_model)

# Print model summary
print(dt_model)

# Predict on test data
dt_pred <- predict(dt_model, newdata = testData, type = "class")

# Evaluate the model using confusion matrix
conf_mat <- confusionMatrix(factor(dt_pred, levels = levels(testData$Diagnosis))
                            , testData$Diagnosis)

# Convert the confusion matrix to a table
cm_table <- as.table(conf_mat$table)

# Create a data frame for ggplot2
cm_df <- as.data.frame(cm_table)

# Plot the confusion matrix
ggplot(data = cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix for Decision Tree", x = "Predicted", 
       y = "Actual")

```


### KMeans Clustering

##### Optimal Cluster Identification - Elbow Method
To get the value for optimal number of clusters required for K-means Clustering, we are using Elbow method.

```{r, fig.width = 5, fig.height = 4}
fviz_nbclust(data[numeric_cols], kmeans, method = "wss") + 
  labs(title = "Elbow Method for Optimal Clusters")
```

##### Gap Statistics
Another method to calcuate optimal number of clusters required for K-means.

```{r, fig.width = 5, fig.height = 4}
gap_stat <- clusGap(data[numeric_cols], FUN = kmeans, nstart = 25, K.max = 10, 
                    B = 50)
fviz_gap_stat(gap_stat)
```

```{r}
# Perform K-Means clustering
set.seed(123)  # For reproducibility
kmeans_result <- kmeans(data[numeric_cols], centers = 8, nstart = 25)  

# Visualize K-Means clusters
fviz_cluster(kmeans_result, data = data[numeric_cols],
             geom = "point", stand = FALSE,
             ellipse.type = "convex", ggtheme = theme_minimal())

```

### Hierarchical Clustering
```{r, fig.width=17, fig.height=10}
# Perform hierarchical clustering
dist_matrix <- dist(data[numeric_cols])  # Compute the distance matrix
hclust_result <- hclust(dist_matrix, method = "ward.D2")

# Dendrogram visualization
plot(hclust_result, labels = data$Diagnosis, main = "Dendrogram", sub = "", 
     xlab = "", ylab = "", cex.main = 2, cex = 0.5 )
rect.hclust(hclust_result, k = 8, border = "red")
```

## MODEL EVALUATIONS
```{r}
# Function to calculate MSE
calculate_mse <- function(predictions, actual) {
  mean((predictions != actual)^2)
}

# Evaluation for Random Forest
rf_accuracy <- sum(rf_pred == testData$Diagnosis) / nrow(testData)
rf_mse <- calculate_mse(rf_pred, testData$Diagnosis)
print(paste("Random Forest Accuracy:", round(rf_accuracy, 4)))
print(paste("Random Forest MSE:", round(rf_mse, 4)))

# Evaluation for Support Vector Machine
svm_accuracy <- sum(svm_pred == testData$Diagnosis) / nrow(testData)
svm_mse <- calculate_mse(svm_pred, testData$Diagnosis)
print(paste("SVM Accuracy:", round(svm_accuracy, 4)))
print(paste("SVM MSE:", round(svm_mse, 4)))

# Evaluation for Multinomial Logistic Regression
log_accuracy <- sum(log_pred == testData$Diagnosis) / nrow(testData)
log_mse <- calculate_mse(log_pred, testData$Diagnosis)
print(paste("Logistic Regression Accuracy:", round(log_accuracy, 4)))
print(paste("Logistic Regression MSE:", round(log_mse, 4)))

# Evaluation for Decision Tree
dt_accuracy <- sum(dt_pred == testData$Diagnosis) / nrow(testData)
dt_mse <- calculate_mse(dt_pred, testData$Diagnosis)
print(paste("Decision Tree Accuracy:", round(dt_accuracy, 4)))
print(paste("Decision Tree MSE:", round(dt_mse, 4)))

```

```{r}
# Calculating Within-Cluster Sum of Squares (WSS)
wss <- kmeans_result$tot.withinss
print(paste("K-Means Within-Cluster Sum of Squares:", round(wss, 4)))

# Silhouette Width
library(cluster)
silhouette_result <- silhouette(kmeans_result$cluster, dist(data[numeric_cols]))
avg_silhouette_width <- mean(silhouette_result[, 3])
print(paste("Average Silhouette Width:", round(avg_silhouette_width, 4)))

#Cluster to Diagnosis table
cluster_labels <- factor(kmeans_result$cluster)
table_clusters <- table(cluster_labels, data$Diagnosis)
print("Cluster to Diagnosis Table:")
print(table_clusters)
```

```{r}
data$Kmeans_cluster <- kmeans_result$cluster
# Function to convert cluster labels to most common true labels in each cluster
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

convert_to_labels <- function(data, true_label, cluster_label) {
  label_mapping <- data %>%
    group_by(!!sym(cluster_label)) %>%
    summarise(TrueLabel = Mode(!!sym(true_label)), .groups = 'drop') %>%
    pull(TrueLabel)

  converted_labels <- label_mapping[data[[cluster_label]]]
  return(converted_labels)
}

# Convert cluster numbers to actual labels
data$MappedClusterLabels <- 
  convert_to_labels(data, "Diagnosis", "Kmeans_cluster")

# Calculate confusion matrix
conf_matrix <- confusionMatrix(as.factor(data$MappedClusterLabels), 
                               as.factor(data$Diagnosis))

# Convert the confusion matrix to a table
cm_table <- as.table(conf_matrix$table)

# Create a data frame for ggplot2
cm_df <- as.data.frame(cm_table)

# Plot the confusion matrix
ggplot(data = cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(title = "Confusion Matrix for K-Means", x = "Predicted", y = "Actual")

cat("Accuracy of K-means:", conf_matrix$overall['Accuracy'], "\n")
